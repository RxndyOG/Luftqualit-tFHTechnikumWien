{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7114a040",
   "metadata": {},
   "source": [
    "### Luftqualität Database upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3245f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, ReplaceOne\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "# load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "mongo_user = os.getenv(\"MONGO_USER\")\n",
    "mongo_pass = os.getenv(\"MONGO_PASS\")\n",
    "mongo_host = os.getenv(\"MONGO_HOST\")\n",
    "mongo_port = os.getenv(\"MONGO_PORT\")\n",
    "mongo_db   = os.getenv(\"MONGO_DB\")\n",
    "\n",
    "# build connection URI and connect\n",
    "uri = f\"mongodb://{mongo_user}:{mongo_pass}@{mongo_host}:{mongo_port}/\"\n",
    "client = MongoClient(uri)\n",
    "db = client[mongo_db]\n",
    "\n",
    "# Now `db` can be used to access collections. The next cell contains upload helpers and the upload loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288461c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Data\\LuftqualitätStateOfGlobalAirAustria.csv\n",
      "Loaded with opts: {'encoding': 'utf-8', 'sep': ','}\n",
      "No good key fields found — inserting documents (duplicates may error)\n",
      "Inserted 36 documents into state_of_global_air\n",
      "Loading: Data\\LuftqualitätStateOfGlobalAirEurope.csv\n",
      "Loaded with opts: {'encoding': 'utf-8', 'sep': ';'}\n",
      "No good key fields found — inserting documents (duplicates may error)\n",
      "Inserted 852 documents into state_of_global_air\n",
      "Loading: Data\\LuftqualitätStateOfGlobalAirWorld.csv\n",
      "Loaded with opts: {'encoding': 'utf-8', 'sep': ';'}\n",
      "No good key fields found — inserting documents (duplicates may error)\n",
      "Inserted 6972 documents into state_of_global_air\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "def _guess_country_field(columns):\n",
    "    candidates = ['Country','country','Country Name','Entity','Location','State','Region','Country/Region']\n",
    "    for c in candidates:\n",
    "        if c in columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _guess_year_field(columns):\n",
    "    candidates = ['Year','year','TIME','Year_']\n",
    "    for c in candidates:\n",
    "        if c in columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def load_and_upload(csv_path, collection_name='state_of_global_air', region=None, chunk_size=1000):\n",
    "    from pandas.errors import ParserError\n",
    "    print(f'Loading: {csv_path}')\n",
    "    # Try several common encodings and delimiters to handle regional CSV formats\n",
    "    read_attempts = [\n",
    "        {'encoding': 'utf-8', 'sep': ','},\n",
    "        {'encoding': 'utf-8', 'sep': ';'},\n",
    "        {'encoding': 'latin1', 'sep': ','},\n",
    "        {'encoding': 'latin1', 'sep': ';'},\n",
    "        {'encoding': 'utf-8', 'sep': ',', 'engine': 'python', 'on_bad_lines': 'skip'},\n",
    "        {'encoding': 'latin1', 'sep': ';', 'engine': 'python', 'on_bad_lines': 'skip'},\n",
    "    ]\n",
    "    df = None\n",
    "    last_err = None\n",
    "    for opts in read_attempts:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, low_memory=False, **opts)\n",
    "            print(f'Loaded with opts: {opts}')\n",
    "            break\n",
    "        except ParserError as pe:\n",
    "            last_err = pe\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if df is None:\n",
    "        print('Failed to parse CSV using multiple strategies. Last error:')\n",
    "        print(repr(last_err))\n",
    "        # show first 5 raw lines to help debugging\n",
    "        try:\n",
    "            with open(csv_path, 'rb') as f:\n",
    "                sample = f.read(1024).decode('utf-8', errors='replace')\n",
    "            print('File sample (first 1024 bytes, utf-8 replace):')\n",
    "            print(sample)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    # normalize missing values to None for MongoDB\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    if region is not None:\n",
    "        if 'region' not in df.columns:\n",
    "            df['region'] = region\n",
    "    records = df.to_dict(orient='records')\n",
    "    coll = db[collection_name]\n",
    "\n",
    "    country_field = _guess_country_field(df.columns)\n",
    "    year_field = _guess_year_field(df.columns)\n",
    "\n",
    "    if country_field and year_field:\n",
    "        print(f'Upserting using keys: {country_field}, {year_field} (bulk)')\n",
    "        ops = []\n",
    "        written = 0\n",
    "        for rec in records:\n",
    "            filter_doc = {country_field: rec.get(country_field), year_field: rec.get(year_field)}\n",
    "            ops.append(ReplaceOne(filter_doc, rec, upsert=True))\n",
    "            if len(ops) >= chunk_size:\n",
    "                res = coll.bulk_write(ops, ordered=False)\n",
    "                ops = []\n",
    "        if ops:\n",
    "            coll.bulk_write(ops, ordered=False)\n",
    "        print(f'Upsert completed for {csv_path}')\n",
    "    else:\n",
    "        # fallback to insert_many; ignore duplicate key errors if they happen\n",
    "        print('No good key fields found — inserting documents (duplicates may error)')\n",
    "        try:\n",
    "            if records:\n",
    "                coll.insert_many(records, ordered=False)\n",
    "                print(f'Inserted {len(records)} documents into {collection_name}')\n",
    "            else:\n",
    "                print('No records found in file')\n",
    "        except BulkWriteError as bwe:\n",
    "            print('Bulk write error (some inserts may have failed due to duplicates):')\n",
    "            print(bwe.details)\n",
    "\n",
    "# Helper to determine region name from filename\n",
    "def _region_from_filename(fname):\n",
    "    lowered = fname.lower()\n",
    "    if 'austria' in lowered:\n",
    "        return 'Austria'\n",
    "    if 'europe' in lowered:\n",
    "        return 'Europe'\n",
    "    if 'world' in lowered:\n",
    "        return 'World'\n",
    "    return None\n",
    "\n",
    "# Main: iterate over CSVs in Data/ and upload\n",
    "data_dir = 'Data'\n",
    "if not os.path.isdir(data_dir):\n",
    "    print('Data directory not found; please ensure the CSVs exist in a folder named `Data`')\n",
    "else:\n",
    "    files = [f for f in os.listdir(data_dir) if f.startswith('LuftqualitätStateOfGlobalAir') and f.lower().endswith('.csv')]\n",
    "    if not files:\n",
    "        print('No matching CSV files found in Data/. Make sure the CSVs are in the `Data` directory.')\n",
    "    else:\n",
    "        for fname in files:\n",
    "            full = os.path.join(data_dir, fname)\n",
    "            region = _region_from_filename(fname)\n",
    "            load_and_upload(full, collection_name='state_of_global_air', region=region)\n",
    "        print('All files processed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
