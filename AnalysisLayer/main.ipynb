{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2015b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acbd282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .appName(\"air-traffic-year-join-ml\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23279a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAFFIC_CSV_PATH = \"../DataStorageLayer/export/ExportVerkehr2.csv\"\n",
    "AIR_CSV_PATH     = \"../DataStorageLayer/export/ExportSchadstoff.csv\"\n",
    "\n",
    "CSV_SEP = \";\"\n",
    "HAS_HEADER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8854d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic rows: 10 cols: 31\n",
      "air rows: 17630 cols: 11\n",
      "+------------------------+-----------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------------------+\n",
      "|_id                     |Bundesland       |1980  |1990  |2000  |2001  |2002  |2003  |2004  |2005  |2006  |2007  |2008  |2009  |2010  |2011  |2012  |2013  |2014  |2015  |2016  |2017  |2018  |2019  |2020  |2021  |2022  |2023  |2024  |Unnamed: 28|_imported_at           |\n",
      "+------------------------+-----------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------------------+\n",
      "|6960fb9f07813795b1f3129f|Burgenland       |277000|393000|551000|565000|548000|559000|570000|575000|581000|586000|591000|599000|609000|618000|626000|633000|639000|641000|649000|659000|668000|675000|681000|684000|679000|683000|692000|NULL       |2026-01-09 12:59:11.483|\n",
      "|6960fb9f07813795b1f312a0|Kaernten         |296000|390000|554000|567000|516000|524000|534000|541000|547000|552000|557000|568000|580000|591000|600000|609000|613000|616000|623000|632000|641000|648000|654000|656000|654000|656000|661000|NULL       |2026-01-09 12:59:11.483|\n",
      "|6960fb9f07813795b1f312a1|Niederoesterreich|307000|431000|560000|570000|550000|561000|566000|570000|574000|579000|583000|592000|603000|612000|619000|623000|627000|628000|633000|641000|649000|654000|659000|661000|655000|657000|662000|NULL       |2026-01-09 12:59:11.483|\n",
      "|6960fb9f07813795b1f312a2|Oberoesterreich  |305000|416000|531000|542000|522000|531000|535000|541000|546000|552000|558000|568000|578000|589000|599000|604000|608000|609000|614000|622000|630000|636000|641000|643000|639000|641000|645000|NULL       |2026-01-09 12:59:11.483|\n",
      "|6960fb9f07813795b1f312a3|Salzburg         |321000|418000|479000|487000|474000|483000|482000|484000|488000|493000|498000|508000|520000|527000|534000|543000|546000|546000|551000|557000|564000|567000|570000|572000|569000|569000|573000|NULL       |2026-01-09 12:59:11.483|\n",
      "+------------------------+-----------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------------------+\n",
      "only showing top 5 rows\n",
      "+------------------------+------+----------+-------+--------+-------------------+--------------------+----------+----+------------+-----------------------+\n",
      "|_id                     |Region|Schadstoff|Einheit|NFR_Code|Trendbericht_Sektor|Quelle              |Datenstand|Jahr| Werte      |_imported_at           |\n",
      "+------------------------+------+----------+-------+--------+-------------------+--------------------+----------+----+------------+-----------------------+\n",
      "|6960fae007813795b1f2cdc0|AT    |NOX       |t      |0       |Gesamt             |OLI 2024 (1990-2023)|45703     |1990| 215.855,04 |2026-01-09 12:56:00.374|\n",
      "|6960fae007813795b1f2cdc1|AT    |NOX       |t      |0       |Gesamt             |OLI 2024 (1990-2023)|45703     |1991| 225.617,42 |2026-01-09 12:56:00.374|\n",
      "|6960fae007813795b1f2cdc2|AT    |NOX       |t      |0       |Gesamt             |OLI 2024 (1990-2023)|45703     |1992| 213.983,87 |2026-01-09 12:56:00.374|\n",
      "|6960fae007813795b1f2cdc3|AT    |NOX       |t      |0       |Gesamt             |OLI 2024 (1990-2023)|45703     |1993| 208.070,23 |2026-01-09 12:56:00.374|\n",
      "|6960fae007813795b1f2cdc4|AT    |NOX       |t      |0       |Gesamt             |OLI 2024 (1990-2023)|45703     |1994| 199.885,73 |2026-01-09 12:56:00.374|\n",
      "+------------------------+------+----------+-------+--------+-------------------+--------------------+----------+----+------------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "traffic_raw = (spark.read\n",
    "               .option(\"header\", str(HAS_HEADER).lower())\n",
    "               .option(\"sep\", CSV_SEP)\n",
    "               .option(\"inferSchema\", \"false\")\n",
    "               .csv(TRAFFIC_CSV_PATH))\n",
    "\n",
    "air_raw = (spark.read\n",
    "           .option(\"header\", str(HAS_HEADER).lower())\n",
    "           .option(\"sep\", CSV_SEP)\n",
    "           .option(\"inferSchema\", \"false\")\n",
    "           .csv(AIR_CSV_PATH))\n",
    "\n",
    "print(\"traffic rows:\", traffic_raw.count(), \"cols:\", len(traffic_raw.columns))\n",
    "print(\"air rows:\", air_raw.count(), \"cols:\", len(air_raw.columns))\n",
    "\n",
    "traffic_raw.show(5, truncate=False)\n",
    "air_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a72d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic columns: ['_id', 'Bundesland', '1980', '1990', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', 'Unnamed: 28', '_imported_at']\n",
      "Air columns: ['_id', 'Region', 'Schadstoff', 'Einheit', 'NFR_Code', 'Trendbericht_Sektor', 'Quelle', 'Datenstand', 'Jahr', 'Werte', '_imported_at']\n"
     ]
    }
   ],
   "source": [
    "traffic_raw = traffic_raw.toDF(*[c.strip() for c in traffic_raw.columns])\n",
    "air_raw = air_raw.toDF(*[c.strip() for c in air_raw.columns])\n",
    "\n",
    "print(\"Traffic columns:\", traffic_raw.columns)\n",
    "print(\"Air columns:\", air_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc12d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_de_number_safe(df, colname: str):\n",
    "    \"\"\"\n",
    "    Robust: ' 215.855,04 ' -> 215855.04 (double)\n",
    "    ' NA ' / '' / '-' -> NULL\n",
    "    Ungültige Werte -> NULL (durch SQL try_cast)\n",
    "    \"\"\"\n",
    "    tmp = f\"__{colname}_norm\"\n",
    "\n",
    "    # 1) trim + string\n",
    "    df = df.withColumn(tmp, F.trim(F.col(colname).cast(\"string\")))\n",
    "\n",
    "    # 2) NA/Noise -> NULL\n",
    "    df = df.withColumn(\n",
    "        tmp,\n",
    "        F.when(\n",
    "            F.col(tmp).isNull()\n",
    "            | (F.col(tmp) == \"\")\n",
    "            | (F.lower(F.col(tmp)).isin(\"na\", \"n/a\", \"null\", \"none\", \"-\", \"—\")),\n",
    "            F.lit(None),\n",
    "        ).otherwise(F.col(tmp))\n",
    "    )\n",
    "\n",
    "    # 3) de-DE cleanup\n",
    "    df = df.withColumn(tmp, F.regexp_replace(F.col(tmp), r\"\\.\", \"\"))  # Tausenderpunkte raus\n",
    "    df = df.withColumn(tmp, F.regexp_replace(F.col(tmp), r\",\", \".\"))  # Komma -> Punkt\n",
    "\n",
    "    # 4) try_cast (SQL) -> ungültig wird NULL statt Fehler\n",
    "    df = df.withColumn(colname, F.expr(f\"try_cast({tmp} as double)\")).drop(tmp)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40af225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\FHTechnikum\\Semester 5\\BigData\\BigDataProjekt\\venv\\Lib\\site-packages\\pyspark\\sql\\classic\\column.py:359: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+------------+\n",
      "|Region_src|Region    |YEAR|ROAD_TRAFFIC|\n",
      "+----------+----------+----+------------+\n",
      "|Burgenland|Burgenland|1980|277000.0    |\n",
      "|Burgenland|Burgenland|1990|393000.0    |\n",
      "|Burgenland|Burgenland|2000|551000.0    |\n",
      "|Burgenland|Burgenland|2001|565000.0    |\n",
      "|Burgenland|Burgenland|2002|548000.0    |\n",
      "|Burgenland|Burgenland|2003|559000.0    |\n",
      "|Burgenland|Burgenland|2004|570000.0    |\n",
      "|Burgenland|Burgenland|2005|575000.0    |\n",
      "|Burgenland|Burgenland|2006|581000.0    |\n",
      "|Burgenland|Burgenland|2007|586000.0    |\n",
      "|Burgenland|Burgenland|2008|591000.0    |\n",
      "|Burgenland|Burgenland|2009|599000.0    |\n",
      "|Burgenland|Burgenland|2010|609000.0    |\n",
      "|Burgenland|Burgenland|2011|618000.0    |\n",
      "|Burgenland|Burgenland|2012|626000.0    |\n",
      "|Burgenland|Burgenland|2013|633000.0    |\n",
      "|Burgenland|Burgenland|2014|639000.0    |\n",
      "|Burgenland|Burgenland|2015|641000.0    |\n",
      "|Burgenland|Burgenland|2016|649000.0    |\n",
      "|Burgenland|Burgenland|2017|659000.0    |\n",
      "+----------+----------+----+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Traffic2: Wide -> Long\n",
    "# ---------------------------\n",
    "TRAFFIC_REGION_SRC_COL = \"Bundesland\"            # ExportVerkehr2\n",
    "TRAFFIC_VALUE_COL      = \"ROAD_TRAFFIC\"\n",
    "TRAFFIC_YEAR_COL       = \"YEAR\"\n",
    "\n",
    "# year columns are digits like \"1990\", \"2001\", ...\n",
    "year_cols = [c for c in traffic_raw.columns if c.isdigit()]\n",
    "year_cols_sorted = sorted(year_cols, key=lambda x: int(x))\n",
    "\n",
    "if not year_cols_sorted:\n",
    "    raise ValueError(\"Keine Jahres-Spalten (digit columns) in ExportVerkehr2 gefunden.\")\n",
    "\n",
    "# stack expression: stack(n, '1990', `1990`, '1991', `1991`, ...) as (YEAR, ROAD_TRAFFIC)\n",
    "stack_args = \", \".join([f\"'{y}', `{y}`\" for y in year_cols_sorted])\n",
    "stack_expr = f\"stack({len(year_cols_sorted)}, {stack_args}) as ({TRAFFIC_YEAR_COL}, {TRAFFIC_VALUE_COL})\"\n",
    "\n",
    "traffic_long = (traffic_raw\n",
    "    .select(F.col(TRAFFIC_REGION_SRC_COL).alias(\"Region_src\"), F.expr(stack_expr))\n",
    ")\n",
    "\n",
    "# Cast year and value\n",
    "traffic_long = traffic_long.withColumn(TRAFFIC_YEAR_COL, F.col(TRAFFIC_YEAR_COL).cast(\"int\"))\n",
    "traffic_long = cast_de_number_safe(traffic_long, TRAFFIC_VALUE_COL)\n",
    "\n",
    "# Region name harmonization between files\n",
    "# - Verkehr2: Niederoesterreich / Oberoesterreich / OESTERREICH\n",
    "# - Schadstoff: Niederoestereich / Oberoestereich / AT\n",
    "REGION_MAP = {\n",
    "    \"Niederoesterreich\": \"Niederoestereich\",\n",
    "    \"Oberoesterreich\": \"Oberoestereich\",\n",
    "    \"OESTERREICH\": \"AT\",\n",
    "}\n",
    "traffic_long = traffic_long.withColumn(\n",
    "    \"Region\",\n",
    "    F.coalesce(\n",
    "        F.create_map([F.lit(x) for kv in REGION_MAP.items() for x in kv]).getItem(F.col(\"Region_src\")),\n",
    "        F.col(\"Region_src\")\n",
    "    )\n",
    ")\n",
    "\n",
    "traffic_long.select(\"Region_src\", \"Region\", TRAFFIC_YEAR_COL, TRAFFIC_VALUE_COL).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819f7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate (if duplicates exist)\n",
    "traffic_year = (traffic_long\n",
    "    .groupBy(TRAFFIC_YEAR_COL, \"Region\")\n",
    "    .agg(F.avg(F.col(TRAFFIC_VALUE_COL)).alias(\"traffic_road_traffic_avg\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78a341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Air (Schadstoff)\n",
    "# ---------------------------\n",
    "AIR_REGION_COL    = \"Region\"\n",
    "AIR_YEAR_COL      = \"Jahr\"\n",
    "AIR_POLLUTANT_COL = \"Schadstoff\"\n",
    "AIR_VALUE_COL     = \"Werte\"\n",
    "\n",
    "air = air_raw.withColumn(AIR_YEAR_COL, F.col(AIR_YEAR_COL).cast(\"int\"))\n",
    "air = cast_de_number_safe(air, AIR_VALUE_COL)\n",
    "\n",
    "# If PM2.5 exists, rename to spark-safe column name\n",
    "if \"PM2.5\" in air.columns:\n",
    "    air = air.withColumnRenamed(\"PM2.5\", \"PM2_5\")\n",
    "\n",
    "\n",
    "PLOTS_DIR = \"../DataOutputLayer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610c08d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037dcbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions (intersection): ['AT', 'Burgenland', 'Kaernten', 'Niederoestereich', 'Oberoestereich', 'Salzburg', 'Steiermark', 'Tirol', 'Vorarlberg', 'Wien']\n"
     ]
    }
   ],
   "source": [
    "pollutants = [\"NOX\", \"SO2\", \"NMVOC\", \"NH3\", \"PM2_5\"]\n",
    "FEATURE_COLS = [\"traffic_road_traffic_avg\", \"year_feature\"]\n",
    "\n",
    "IMPUTE_MISSING_TRAFFIC_WITH_ZERO = True\n",
    "\n",
    "# Regions present in both datasets\n",
    "regions_air = [r[\"Region\"] for r in air.select(F.col(AIR_REGION_COL).alias(\"Region\")).distinct().collect()]\n",
    "regions_traffic = [r[\"Region\"] for r in traffic_year.select(\"Region\").distinct().collect()]\n",
    "regions = sorted(set(regions_air).intersection(set(regions_traffic)))\n",
    "\n",
    "print(\"Regions (intersection):\", regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511562d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "REGION: AT\n",
      "==============================\n",
      "  NOX: RMSE=8264.32, R2=0.710\n",
      "  SO2: RMSE=1538.20, R2=0.929\n",
      "  NMVOC: RMSE=4876.90, R2=0.957\n",
      "  NH3: RMSE=5721.72, R2=0.841\n",
      "  PM2_5: RMSE=543.38, R2=0.772\n",
      "\n",
      "==============================\n",
      "REGION: Burgenland\n",
      "==============================\n",
      "  NOX: RMSE=101.55, R2=0.853\n",
      "  SO2: RMSE=25.72, R2=0.975\n",
      "  NMVOC: RMSE=98.61, R2=0.977\n",
      "  NH3: RMSE=12.05, R2=0.981\n",
      "  PM2_5: RMSE=10.17, R2=0.938\n",
      "\n",
      "==============================\n",
      "REGION: Kaernten\n",
      "==============================\n",
      "  NOX: RMSE=171.13, R2=0.906\n",
      "  SO2: RMSE=93.76, R2=0.923\n",
      "  NMVOC: RMSE=230.86, R2=0.967\n",
      "  NH3: RMSE=19.10, R2=0.659\n",
      "  PM2_5: RMSE=13.43, R2=0.970\n",
      "\n",
      "==============================\n",
      "REGION: Niederoestereich\n",
      "==============================\n",
      "  NOX: RMSE=617.01, R2=0.895\n",
      "  SO2: RMSE=172.45, R2=0.982\n",
      "  NMVOC: RMSE=669.73, R2=0.982\n",
      "  NH3: RMSE=80.73, R2=0.938\n",
      "  PM2_5: RMSE=44.13, R2=0.957\n",
      "\n",
      "==============================\n",
      "REGION: Oberoestereich\n",
      "==============================\n",
      "  NOX: RMSE=499.99, R2=0.886\n",
      "  SO2: RMSE=271.82, R2=0.889\n",
      "  NMVOC: RMSE=678.55, R2=0.968\n",
      "  NH3: RMSE=54.45, R2=0.795\n",
      "  PM2_5: RMSE=48.16, R2=0.962\n",
      "\n",
      "==============================\n",
      "REGION: Salzburg\n",
      "==============================\n",
      "  NOX: RMSE=168.79, R2=0.869\n",
      "  SO2: RMSE=47.58, R2=0.952\n",
      "  NMVOC: RMSE=157.01, R2=0.973\n",
      "  NH3: RMSE=12.25, R2=0.948\n",
      "  PM2_5: RMSE=8.92, R2=0.979\n",
      "\n",
      "==============================\n",
      "REGION: Steiermark\n",
      "==============================\n",
      "  NOX: RMSE=445.71, R2=0.866\n",
      "  SO2: RMSE=205.73, R2=0.927\n",
      "  NMVOC: RMSE=449.18, R2=0.968\n",
      "  NH3: RMSE=33.65, R2=0.924\n",
      "  PM2_5: RMSE=30.81, R2=0.972\n",
      "\n",
      "==============================\n",
      "REGION: Tirol\n",
      "==============================\n",
      "  NOX: RMSE=215.68, R2=0.825\n",
      "  SO2: RMSE=67.76, R2=0.935\n",
      "  NMVOC: RMSE=209.00, R2=0.981\n",
      "  NH3: RMSE=14.71, R2=0.914\n",
      "  PM2_5: RMSE=16.94, R2=0.946\n",
      "\n",
      "==============================\n",
      "REGION: Vorarlberg\n",
      "==============================\n",
      "  NOX: RMSE=124.21, R2=0.809\n",
      "  SO2: RMSE=27.51, R2=0.954\n",
      "  NMVOC: RMSE=139.04, R2=0.964\n",
      "  NH3: RMSE=7.30, R2=0.919\n",
      "  PM2_5: RMSE=5.75, R2=0.958\n",
      "\n",
      "==============================\n",
      "REGION: Wien\n",
      "==============================\n",
      "  NOX: RMSE=289.58, R2=0.951\n",
      "  SO2: RMSE=206.41, R2=0.781\n",
      "  NMVOC: RMSE=391.02, R2=0.972\n",
      "  NH3: RMSE=10.02, R2=0.922\n",
      "  PM2_5: RMSE=13.05, R2=0.988\n"
     ]
    }
   ],
   "source": [
    "all_metrics = []\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for region in regions:\n",
    "    # Skip national if you want only provinces/cities; comment out if you want AT too.\n",
    "    # if region == \"AT\":\n",
    "    #     continue\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"REGION:\", region)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # Filter air for region\n",
    "    air_r = air.filter(F.trim(F.col(AIR_REGION_COL)) == F.lit(region))\n",
    "\n",
    "    # Air agg + pivot\n",
    "    air_year = (air_r\n",
    "        .groupBy(AIR_YEAR_COL, AIR_POLLUTANT_COL)\n",
    "        .agg(F.avg(F.col(AIR_VALUE_COL)).alias(\"poll_value_avg\"))\n",
    "    )\n",
    "\n",
    "    air_year_pivot = (air_year\n",
    "        .groupBy(AIR_YEAR_COL)\n",
    "        .pivot(AIR_POLLUTANT_COL)\n",
    "        .agg(F.first(\"poll_value_avg\"))\n",
    "    )\n",
    "\n",
    "    # Region-specific traffic\n",
    "    traffic_r = traffic_year.filter(F.col(\"Region\") == F.lit(region))\n",
    "\n",
    "    joined = (air_year_pivot\n",
    "        .join(\n",
    "            traffic_r.withColumnRenamed(TRAFFIC_YEAR_COL, \"YEAR_join\"),\n",
    "            air_year_pivot[AIR_YEAR_COL] == F.col(\"YEAR_join\"),\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .drop(\"YEAR_join\")\n",
    "        .withColumn(\"Region\", F.lit(region))\n",
    "    )\n",
    "\n",
    "    if IMPUTE_MISSING_TRAFFIC_WITH_ZERO:\n",
    "        joined = joined.fillna({\"traffic_road_traffic_avg\": 0.0})\n",
    "\n",
    "    # year feature\n",
    "    model_df = joined.withColumn(\"year_feature\", F.col(AIR_YEAR_COL).cast(\"double\"))\n",
    "\n",
    "    # If PM2.5 column appears (rare), rename it\n",
    "    if \"PM2.5\" in model_df.columns:\n",
    "        model_df = model_df.withColumnRenamed(\"PM2.5\", \"PM2_5\")\n",
    "\n",
    "    # Train models for all pollutants available in this region DF\n",
    "    region_metrics = []\n",
    "    for label in pollutants:\n",
    "        if label not in model_df.columns:\n",
    "            continue\n",
    "\n",
    "        data = model_df.select(AIR_YEAR_COL, label, *FEATURE_COLS).dropna(subset=[label] + FEATURE_COLS)\n",
    "\n",
    "        if data.count() < 10:\n",
    "            print(f\"  {label}: zu wenig Daten\")\n",
    "            continue\n",
    "\n",
    "        train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "        assembler = VectorAssembler(inputCols=FEATURE_COLS, outputCol=\"features\")\n",
    "        model = RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=label,\n",
    "            numTrees=200,\n",
    "            maxDepth=8,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        pipe = Pipeline(stages=[assembler, model])\n",
    "        fitted = pipe.fit(train)\n",
    "        preds = fitted.transform(test).select(AIR_YEAR_COL, label, \"prediction\").orderBy(AIR_YEAR_COL)\n",
    "\n",
    "        preds_with_region = preds.withColumn(\"Region\", F.lit(region)).withColumn(\"pollutant\", F.lit(label))\n",
    "        # und in eine Liste sammeln:\n",
    "        all_preds.append(preds_with_region)\n",
    "\n",
    "        rmse = RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\").evaluate(preds)\n",
    "        r2 = RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"r2\").evaluate(preds)\n",
    "\n",
    "        region_metrics.append((region, label, rmse, r2))\n",
    "        all_metrics.append((region, label, rmse, r2))\n",
    "\n",
    "        print(f\"  {label}: RMSE={rmse:.2f}, R2={r2:.3f}\")\n",
    "\n",
    "        # --- plots ---\n",
    "        pdf = preds.toPandas().sort_values(AIR_YEAR_COL)\n",
    "\n",
    "        # timeseries\n",
    "        plt.figure()\n",
    "        plt.plot(pdf[AIR_YEAR_COL], pdf[label], marker=\"o\")\n",
    "        plt.plot(pdf[AIR_YEAR_COL], pdf[\"prediction\"], marker=\"o\")\n",
    "        plt.xlabel(\"Jahr\")\n",
    "        plt.ylabel(label)\n",
    "        plt.title(f\"{region} | {label}: Ist vs Prognose (Test) | R2={r2:.3f}, RMSE={rmse:.2f}\")\n",
    "        plt.legend([\"Ist\", \"Prognose\"])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f\"{region}_{label}_timeseries_test.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # scatter\n",
    "        plt.figure()\n",
    "        plt.scatter(pdf[label], pdf[\"prediction\"])\n",
    "        mn = min(pdf[label].min(), pdf[\"prediction\"].min())\n",
    "        mx = max(pdf[label].max(), pdf[\"prediction\"].max())\n",
    "        plt.plot([mn, mx], [mn, mx])\n",
    "        plt.xlabel(\"Ist\")\n",
    "        plt.ylabel(\"Prognose\")\n",
    "        plt.title(f\"{region} | {label}: Scatter (Test) | R2={r2:.3f}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f\"{region}_{label}_scatter_test.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # Per-region overview plots + metrics CSV\n",
    "    if region_metrics:\n",
    "        res_df = pd.DataFrame(region_metrics, columns=[\"region\", \"pollutant\", \"rmse\", \"r2\"]).sort_values(\"pollutant\")\n",
    "        res_df.to_csv(os.path.join(PLOTS_DIR, f\"{region}_metrics_overview.csv\"), index=False)\n",
    "\n",
    "        # R2 bar\n",
    "        plt.figure()\n",
    "        plt.bar(res_df[\"pollutant\"], res_df[\"r2\"])\n",
    "        plt.xlabel(\"Schadstoff\")\n",
    "        plt.ylabel(\"R2\")\n",
    "        plt.title(f\"{region}: Modellgüte (R2) pro Schadstoff\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f\"{region}_overview_r2.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # RMSE bar\n",
    "        plt.figure()\n",
    "        plt.bar(res_df[\"pollutant\"], res_df[\"rmse\"])\n",
    "        plt.xlabel(\"Schadstoff\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.title(f\"{region}: Fehler (RMSE) pro Schadstoff\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f\"{region}_overview_rmse.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fdb0cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global Aggregates/Plots gespeichert: d:\\FHTechnikum\\Semester 5\\BigData\\BigDataProjekt\\DataOutputLayer\n"
     ]
    }
   ],
   "source": [
    "# Global metrics summary\n",
    "if all_metrics:\n",
    "    all_df = pd.DataFrame(all_metrics, columns=[\"region\", \"pollutant\", \"rmse\", \"r2\"]).sort_values([\"region\",\"pollutant\"])\n",
    "    all_df.to_csv(os.path.join(PLOTS_DIR, \"ALL_regions_metrics.csv\"), index=False)\n",
    "    \n",
    "    # Aggregation pro Schadstoff\n",
    "    agg = (all_df\n",
    "        .groupby(\"pollutant\")\n",
    "        .agg(\n",
    "            r2_mean=(\"r2\", \"mean\"),\n",
    "            r2_median=(\"r2\", \"median\"),\n",
    "            r2_std=(\"r2\", \"std\"),\n",
    "            rmse_mean=(\"rmse\", \"mean\"),\n",
    "            rmse_median=(\"rmse\", \"median\"),\n",
    "            rmse_std=(\"rmse\", \"std\"),\n",
    "            n_regions=(\"region\", \"nunique\")\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"pollutant\")\n",
    "    )\n",
    "    agg.to_csv(os.path.join(PLOTS_DIR, \"ALL_pollutants_agg.csv\"), index=False)\n",
    "\n",
    "    # Plot: Mean R2 pro Schadstoff (mit errorbar = std)\n",
    "    plt.figure()\n",
    "    plt.bar(agg[\"pollutant\"], agg[\"r2_mean\"])\n",
    "    plt.errorbar(agg[\"pollutant\"], agg[\"r2_mean\"], yerr=agg[\"r2_std\"], fmt=\"none\", capsize=4)\n",
    "    plt.xlabel(\"Schadstoff\")\n",
    "    plt.ylabel(\"R2 (Mean ± Std über Regionen)\")\n",
    "    plt.title(\"Global: Modellgüte über alle Regionen\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"GLOBAL_mean_r2_with_std.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Mean RMSE pro Schadstoff (mit errorbar = std)\n",
    "    plt.figure()\n",
    "    plt.bar(agg[\"pollutant\"], agg[\"rmse_mean\"])\n",
    "    plt.errorbar(agg[\"pollutant\"], agg[\"rmse_mean\"], yerr=agg[\"rmse_std\"], fmt=\"none\", capsize=4)\n",
    "    plt.xlabel(\"Schadstoff\")\n",
    "    plt.ylabel(\"RMSE (Mean ± Std über Regionen)\")\n",
    "    plt.title(\"Global: Fehler über alle Regionen\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"GLOBAL_mean_rmse_with_std.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"✅ Global Aggregates/Plots gespeichert:\", os.path.abspath(PLOTS_DIR))\n",
    "else:\n",
    "    print(\"⚠️ Keine globalen Metriken vorhanden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d82ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics:\n",
    "    pivot_r2 = all_df.pivot(index=\"region\", columns=\"pollutant\", values=\"r2\").sort_index()\n",
    "    pivot_r2.to_csv(os.path.join(PLOTS_DIR, \"GLOBAL_r2_matrix.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(pivot_r2.values, aspect=\"auto\")\n",
    "    plt.xticks(range(len(pivot_r2.columns)), pivot_r2.columns, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(pivot_r2.index)), pivot_r2.index)\n",
    "    plt.colorbar(label=\"R2\")\n",
    "    plt.title(\"R2 Heatmap: Regionen × Schadstoffe\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, \"GLOBAL_r2_heatmap.png\"), dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44a120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics:\n",
    "    for pol in sorted(all_df[\"pollutant\"].unique()):\n",
    "        top = all_df[all_df[\"pollutant\"] == pol].sort_values(\"r2\", ascending=False).head(5)\n",
    "        top.to_csv(os.path.join(PLOTS_DIR, f\"TOP5_{pol}_by_r2.csv\"), index=False)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.bar(top[\"region\"], top[\"r2\"])\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.xlabel(\"Region\")\n",
    "        plt.ylabel(\"R2\")\n",
    "        plt.title(f\"Top 5 Regionen nach R2 ({pol})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(PLOTS_DIR, f\"TOP5_{pol}_r2.png\"), dpi=200)\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
